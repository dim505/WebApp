#pragma checksum "C:\Users\bob\Documents\Visual Studio 2015\Projects\razor_pages\WebApp\main_portfolio\Pages\Python.cshtml" "{ff1816ec-aa5e-4d10-87f7-6f4963833460}" "3cd800f691bf0c84812c9c1514e1a77d7d8adcba"
// <auto-generated/>
#pragma warning disable 1591
[assembly: global::Microsoft.AspNetCore.Razor.Hosting.RazorCompiledItemAttribute(typeof(razor_pages.Pages.Pages_Python), @"mvc.1.0.razor-page", @"/Pages/Python.cshtml")]
[assembly:global::Microsoft.AspNetCore.Mvc.RazorPages.Infrastructure.RazorPageAttribute(@"/Pages/Python.cshtml", typeof(razor_pages.Pages.Pages_Python), null)]
namespace razor_pages.Pages
{
    #line hidden
    using System;
    using System.Collections.Generic;
    using System.Linq;
    using System.Threading.Tasks;
    using Microsoft.AspNetCore.Mvc;
    using Microsoft.AspNetCore.Mvc.Rendering;
    using Microsoft.AspNetCore.Mvc.ViewFeatures;
#line 1 "C:\Users\bob\Documents\Visual Studio 2015\Projects\razor_pages\WebApp\main_portfolio\Pages\_ViewImports.cshtml"
using razor_pages;

#line default
#line hidden
    [global::Microsoft.AspNetCore.Razor.Hosting.RazorSourceChecksumAttribute(@"SHA1", @"3cd800f691bf0c84812c9c1514e1a77d7d8adcba", @"/Pages/Python.cshtml")]
    [global::Microsoft.AspNetCore.Razor.Hosting.RazorSourceChecksumAttribute(@"SHA1", @"b0d45e63f997e68bd8ec1dfbd8cb9a4057c4fad5", @"/Pages/_ViewImports.cshtml")]
    public class Pages_Python : global::Microsoft.AspNetCore.Mvc.RazorPages.Page
    {
        #pragma warning disable 1998
        public async override global::System.Threading.Tasks.Task ExecuteAsync()
        {
            BeginContext(57, 2, true);
            WriteLiteral("\r\n");
            EndContext();
#line 6 "C:\Users\bob\Documents\Visual Studio 2015\Projects\razor_pages\WebApp\main_portfolio\Pages\Python.cshtml"
  
    ViewData["Title"] = "Python Wiki Crawler";


#line default
#line hidden
            BeginContext(116, 6156, true);
            WriteLiteral(@"



<div class=""AMS"">




    <h1>
        Python Wiki Crawler

    </h1>


    <p>
        <a href=""https://www.huffpost.com/entry/wikipedia-philosophy_n_1093460""> Legend has it that </a>  95% of all pages are linked to the <a href=""https://en.wikipedia.org/wiki/Philosophy"">
            wikipedia Philosophy Page
        </a>. If you click on the first link of a wikipedia page it will
        lead to an ever broadening subject and reaching general subjects like Mathematics, Science, Language, and Philosophy.
    </p>
    <p>
        I made a Python script to test that! It accepts start and end links. It would try to get to the destination link starting from the starting link.
        From there, it will traverse various pages by following the theory of extracting the first non-parenthesized,
        non-italicized link that does not have any external links or links to the current page. It will also show
        you the path it takes trying to get to the destination page
    </p>
   ");
            WriteLiteral(@" <p>
        If you would like to test my theory
        , <a href=""https://www.xefer.com/wikipedia""> please visit here </a> to confirm my findings.
    </p>




    <button type=""button"" class=""btn btn-info btn-lg"" data-toggle=""modal"" data-target=""#myModal"">Click Here for Code</button>

    <!-- Modal -->
    <div id=""myModal"" class=""modal fade"" role=""dialog"">
        <div class=""modal-dialog"">

            <!-- Modal content-->
            <div class=""modal-content"">
                <div class=""modal-header"">
                    <button type=""button"" class=""close"" data-dismiss=""modal"">&times;</button>
                    <h4 class=""modal-title"">Python Code </h4>
                </div>
                <div class=""modal-body"">
                    <div class=""jumbotron"">
                        <pre class=""embed-responsive"">
import sys
import requests
import time
from bs4 import BeautifulSoup

#if something is wrong with the input, it will generate an error and  it will exit.
try:");
            WriteLiteral(@"
	# the url you are starting at
	st_URL = sys.argv[1]
except:
	#prints error
	print(""Looks Like something is wrong there...Please make sure your URL is correct and try again!"")
	#exits script
	sys.exit()

#if something is wrong with the input, it will generate an error and  it will exit.
try:
	# the URL you want to reach 
	end_url = sys.argv[2]
	
except:
	#prints out error
	print(""Looks Like something is wrong there...Please make sure your URL is correct and try again!"")
	#exits script
	sys.exit()

	

print(""\nIT BOT Starting Now........\n"")
	

#list holding at the URLs visited 
Pages_visited = [st_URL] 


#some commands printing to the terminal

def continue_crawl(Pages_visited, end_url):

    #if last URL visited URL in Crawl is equal to target URL then it reached its destination
    if Pages_visited[-1] == end_url:													
        print(""\n\nWe've reached your specified target url - "" + end_url + ""\n"")
        print(""IT Bot going to sleep Beep Bop Boop......"")
     ");
            WriteLiteral(@"   return False
	#compares if number is visited links has exceeded the max visited links threshold of 50
    elif len(Pages_visited) > 50:
        print(""\nOur search history has the maximum article threshold.\n"")
        print(""IT Bot going to sleep Beep Bop Boop......"")
        return False
    #compares number of sites visited  with set number of sites visited, if there is a duplicate in list, you made a loop
	# thus exiting the search 
    elif len(Pages_visited) != len(set(Pages_visited)):
        print(""\nZoinks o_O!!! We've hit a repeat!!\n"")
        print(""Article "" + Pages_visited[-1][30:] + "" is already on the list.\n"")
        print(""IT Bot going to sleep Beep Bop Boop......"")
        return False
    else:
        return True
		

def Find_link(url):
	#downloads the html file
	response = requests.get(url)
	#saves it as a string
	html = response.text
	#creates a BeautifulSoup object used for extracting data from the html 
	SoupObj = BeautifulSoup(html, 'html.parser')
	#finds ");
            WriteLiteral(@"element with ID mw-content-text, inside that element find sub element with classname mw-parser-output
	content_div = SoupObj.find(id=""mw-content-text"").find(class_=""mw-parser-output"")

	article_name = None
	#finds all P elements in element with class mw-parser-output, only sticks to direct children of selected element
	for element in content_div.find_all(""p"", recursive=False):
		#if it finds a element inside of P, that will become the next visited link
		if element.find(""a"", recursive=False):
			#gets Href value from first A link in mw-parser-output class element
			article_name = element.find(""a"", recursive=False).get('href')
			break
	#if it does not find anything then it was a dead end, returns nothing
	if not article_name:
		return 
	#creates the link
	first_link = 'https://en.wikipedia.org' + article_name
	#returns wiki link
	return first_link

	
#checks to see if it should contine checking for links

while continue_crawl(Pages_visited, end_url):

	print(Pages_visited[-1] + ""\n\n""");
            WriteLiteral(@", end='')
		
		#gets the next linked page from the URL
	first_link = Find_link(Pages_visited[-1])
		#if no links are found then program exits
	if not first_link:
		print(""\nThe article has no links.\n"")
		print(""IT Bot going to sleep Beep Bop Boop......"")
		break
		#adds to total list of sites visitedd
	Pages_visited.append(first_link) 

	#slows down crawl rate so it wont be blocked by wikipedia
	time.sleep(2) 




                     </pre>
                    </div>


                </div>
                <div class=""modal-footer"">
                    <button type=""button"" class=""btn btn-default"" data-dismiss=""modal"">Close</button>
                </div>
            </div>

        </div>
    </div>
    <h4> Here are an animation showing my application:</h4>
    <iframe id=""youtube"" width=""100%"" height=""315"" src=""https://www.youtube.com/embed/rXGZ4QRBmpg"" frameborder=""0"" allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" allowfullscreen></iframe>");
            WriteLiteral("\r\n    </div>");
            EndContext();
        }
        #pragma warning restore 1998
        [global::Microsoft.AspNetCore.Mvc.Razor.Internal.RazorInjectAttribute]
        public global::Microsoft.AspNetCore.Mvc.ViewFeatures.IModelExpressionProvider ModelExpressionProvider { get; private set; }
        [global::Microsoft.AspNetCore.Mvc.Razor.Internal.RazorInjectAttribute]
        public global::Microsoft.AspNetCore.Mvc.IUrlHelper Url { get; private set; }
        [global::Microsoft.AspNetCore.Mvc.Razor.Internal.RazorInjectAttribute]
        public global::Microsoft.AspNetCore.Mvc.IViewComponentHelper Component { get; private set; }
        [global::Microsoft.AspNetCore.Mvc.Razor.Internal.RazorInjectAttribute]
        public global::Microsoft.AspNetCore.Mvc.Rendering.IJsonHelper Json { get; private set; }
        [global::Microsoft.AspNetCore.Mvc.Razor.Internal.RazorInjectAttribute]
        public global::Microsoft.AspNetCore.Mvc.Rendering.IHtmlHelper<MyApp.Namespace.ReactWebStoreModel> Html { get; private set; }
        public global::Microsoft.AspNetCore.Mvc.ViewFeatures.ViewDataDictionary<MyApp.Namespace.ReactWebStoreModel> ViewData => (global::Microsoft.AspNetCore.Mvc.ViewFeatures.ViewDataDictionary<MyApp.Namespace.ReactWebStoreModel>)PageContext?.ViewData;
        public MyApp.Namespace.ReactWebStoreModel Model => ViewData.Model;
    }
}
#pragma warning restore 1591
